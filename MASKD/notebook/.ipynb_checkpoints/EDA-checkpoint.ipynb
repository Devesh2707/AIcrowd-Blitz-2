{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_coco = COCO('/run/media/devesh/Hard Disk/Projects/AI Crowd Blitz 2/MASKD/input/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pycocotools.coco.COCO at 0x7f8127c5a1c0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = list(sorted(train_coco.imgs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_ids = train_coco.getAnnIds(imgIds=img_ids[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotation = train_coco.loadAnns(ann_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coco_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ca0dce114d.jpg'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_coco.loadImgs(img_ids[5])[0]['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = []\n",
    "\n",
    "# for i in range(len(img_ids)):\n",
    "#     img = img_ids[i]\n",
    "#     ann_ids = train_coco.getAnnIds(imgIds=img)\n",
    "#     coco_annotation = train_coco.loadAnns(ann_ids)\n",
    "#     categories.append(coco_annotation[0]['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# pd.Series(categories).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_ids[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class myOwnDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None, train = True):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        if train:\n",
    "            self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        else:\n",
    "            self.ids = list(self.coco.imgs.keys())\n",
    "        self.train = train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            # Own coco file\n",
    "            coco = self.coco\n",
    "            # Image ID\n",
    "            img_id = self.ids[index]\n",
    "            # List: get annotation id from coco\n",
    "            ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "            # Dictionary: target coco_annotation file for an image\n",
    "            coco_annotation = coco.loadAnns(ann_ids)\n",
    "            # path for input image\n",
    "            path = coco.loadImgs(img_id)[0]['file_name']\n",
    "            # open the input image\n",
    "            img = Image.open(os.path.join(self.root, path))\n",
    "            img = np.array(img)/255.0\n",
    "\n",
    "            # number of objects in the image\n",
    "            num_objs = len(coco_annotation)\n",
    "\n",
    "            # Bounding boxes for objects\n",
    "            # In coco format, bbox = [xmin, ymin, width, height]\n",
    "            # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "            boxes = []\n",
    "            for i in range(num_objs):\n",
    "                xmin = coco_annotation[i]['bbox'][0]\n",
    "                ymin = coco_annotation[i]['bbox'][1]\n",
    "                xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "                ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            # Labels (In my case, I only one class: target class or background)\n",
    "            labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "            # Tensorise img_id\n",
    "            img_id = torch.tensor([img_id])\n",
    "            # Size of bbox (Rectangular)\n",
    "            areas = []\n",
    "            for i in range(num_objs):\n",
    "                areas.append(coco_annotation[i]['area'])\n",
    "            areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "            # Iscrowd\n",
    "            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "            # Annotation is in dictionary format\n",
    "            my_annotation = {}\n",
    "            my_annotation[\"boxes\"] = boxes\n",
    "            my_annotation[\"labels\"] = labels\n",
    "            my_annotation[\"image_id\"] = img_id\n",
    "            my_annotation[\"area\"] = areas\n",
    "            my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "\n",
    "            return img, my_annotation\n",
    "        else:\n",
    "            # Own coco file\n",
    "            coco = self.coco\n",
    "            # Image ID\n",
    "            img_id = self.ids[index]\n",
    "            # path for input image\n",
    "            path = coco.loadImgs(img_id)[0]['file_name']\n",
    "            # open the input image\n",
    "            img = Image.open(os.path.join(self.root, path))\n",
    "            img = np.array(img)/255.0\n",
    "            \n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(img)\n",
    "                \n",
    "            # Tensorise img_id\n",
    "            img_id = torch.tensor([img_id])\n",
    "            \n",
    "            return img,img_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# path to your own data and coco file\n",
    "train_data_dir = '/run/media/devesh/Hard Disk/Projects/AI Crowd Blitz 2/MASKD/input/train_images'\n",
    "train_coco = '/run/media/devesh/Hard Disk/Projects/AI Crowd Blitz 2/MASKD/input/train.json'\n",
    "val_data_dir = '/run/media/devesh/Hard Disk/Projects/AI Crowd Blitz 2/MASKD/input/val_images'\n",
    "val_coco = '/run/media/devesh/Hard Disk/Projects/AI Crowd Blitz 2/MASKD/input/val.json'\n",
    "test_data_dir = '/run/media/devesh/Hard Disk/Projects/AI Crowd Blitz 2/MASKD/input/test_images'\n",
    "test_coco = '/run/media/devesh/Hard Disk/Projects/AI Crowd Blitz 2/MASKD/input/test.json'\n",
    "\n",
    "# create own Dataset\n",
    "train_dataset = myOwnDataset(root=train_data_dir,\n",
    "                          annotation=train_coco,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "\n",
    "val_dataset = myOwnDataset(root=val_data_dir,\n",
    "                          annotation=val_coco,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "\n",
    "test_dataset = myOwnDataset(root=test_data_dir,\n",
    "                          annotation=test_coco,\n",
    "                          transforms=get_transform(),\n",
    "                          train = False)\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Batch size\n",
    "train_batch_size = 1\n",
    "\n",
    "# own DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# select device (whether GPU or CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[514.,  68., 846., 510.],\n",
      "        [178., 107., 456., 604.]], device='cuda:0'), 'labels': tensor([1, 1], device='cuda:0'), 'image_id': tensor([82], device='cuda:0'), 'area': tensor([146744., 138166.], device='cuda:0'), 'iscrowd': tensor([0, 0], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "#testing dataloader\n",
    "i = 0\n",
    "\n",
    "for imgs, annotations in data_loader:\n",
    "    i += 1\n",
    "    if i % 2 == 0:\n",
    "        break\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 classes; Only target class or background\n",
    "num_classes = 3\n",
    "num_epochs = 5\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devesh/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/opt/conda/conda-bld/pytorch_1587428207430/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 100/679, Loss: {'loss_classifier': tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0687, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 1, Iteration: 200/679, Loss: {'loss_classifier': tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.2062, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 1, Iteration: 300/679, Loss: {'loss_classifier': tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.3219, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0058, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 1, Iteration: 400/679, Loss: {'loss_classifier': tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0276, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0036, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 1, Iteration: 500/679, Loss: {'loss_classifier': tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.1775, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0996, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 1, Iteration: 600/679, Loss: {'loss_classifier': tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0748, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0106, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 1, Iteration: 679/679, Loss: {'loss_classifier': tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0656, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0338, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 2, Iteration: 100/679, Loss: {'loss_classifier': tensor(1.5422, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.2061, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0236, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0165, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 2, Iteration: 200/679, Loss: {'loss_classifier': tensor(3.2243, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(1.7377, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 2, Iteration: 300/679, Loss: {'loss_classifier': tensor(20.8202, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(9.8986, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.2458, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 2, Iteration: 400/679, Loss: {'loss_classifier': tensor(14831.7344, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(1461.4076, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(1.9079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(28.4900, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 2, Iteration: 500/679, Loss: {'loss_classifier': tensor(116.8409, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(7.6256, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(1.7560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(12.3224, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 2, Iteration: 600/679, Loss: {'loss_classifier': tensor(100.9039, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(330.6766, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(99.1570, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(153.8319, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 2, Iteration: 679/679, Loss: {'loss_classifier': tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2278, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.1499, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 3, Iteration: 100/679, Loss: {'loss_classifier': tensor(3512.8569, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(789.9070, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0054, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 3, Iteration: 200/679, Loss: {'loss_classifier': tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(3.8328e-05, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.4957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0023, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 3, Iteration: 300/679, Loss: {'loss_classifier': tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(6.2151e-05, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.4241, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0020, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 3, Iteration: 400/679, Loss: {'loss_classifier': tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(7.9965e-05, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.3866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0132, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 3, Iteration: 500/679, Loss: {'loss_classifier': tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0862, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.4056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 3, Iteration: 600/679, Loss: {'loss_classifier': tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.3405, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0161, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 3, Iteration: 679/679, Loss: {'loss_classifier': tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.3349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0146, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 4, Iteration: 100/679, Loss: {'loss_classifier': tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.3226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0347, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 4, Iteration: 200/679, Loss: {'loss_classifier': tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0125, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2704, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0159, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 4, Iteration: 300/679, Loss: {'loss_classifier': tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0016, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 4, Iteration: 400/679, Loss: {'loss_classifier': tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0346, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0041, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 4, Iteration: 500/679, Loss: {'loss_classifier': tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2039, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0020, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 4, Iteration: 600/679, Loss: {'loss_classifier': tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2097, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0037, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 4, Iteration: 679/679, Loss: {'loss_classifier': tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0265, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.3473, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 5, Iteration: 100/679, Loss: {'loss_classifier': tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1956, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0121, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 5, Iteration: 200/679, Loss: {'loss_classifier': tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0058, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 5, Iteration: 300/679, Loss: {'loss_classifier': tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0058, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 5, Iteration: 400/679, Loss: {'loss_classifier': tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0024, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 5, Iteration: 500/679, Loss: {'loss_classifier': tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0044, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0180, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 5, Iteration: 600/679, Loss: {'loss_classifier': tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0114, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Epoch: 5, Iteration: 679/679, Loss: {'loss_classifier': tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(0.0886, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.2750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "###### parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "len_dataloader = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    for imgs, annotations in train_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.type(torch.FloatTensor).to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i%100 == 0) or (i == 679):\n",
    "            print(f'Epoch: {epoch+1}, Iteration: {i}/{len_dataloader}, Loss: {loss_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = list(img.to(device) for img in imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = my_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.9373, 0.9333, 0.9216,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9294, 0.9294, 0.9373,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9373, 0.9098, 0.8392,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.6235, 0.6196, 0.5961,  ..., 0.5333, 0.5020, 0.5373],\n",
       "          [0.6118, 0.6078, 0.6000,  ..., 0.4902, 0.4039, 0.4902],\n",
       "          [0.5961, 0.5804, 0.6039,  ..., 0.3647, 0.3451, 0.4118]],\n",
       " \n",
       "         [[0.9647, 0.9608, 0.9451,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9569, 0.9569, 0.9608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9647, 0.9373, 0.8549,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.6118, 0.6157, 0.5922,  ..., 0.4863, 0.4549, 0.4784],\n",
       "          [0.6000, 0.5961, 0.5882,  ..., 0.4510, 0.3569, 0.4314],\n",
       "          [0.5882, 0.5725, 0.5922,  ..., 0.3176, 0.2902, 0.3490]],\n",
       " \n",
       "         [[1.0000, 1.0000, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9961, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 0.9765, 0.8980,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.6392, 0.6392, 0.6157,  ..., 0.4863, 0.4627, 0.4980],\n",
       "          [0.6275, 0.6235, 0.6157,  ..., 0.4549, 0.3569, 0.4431],\n",
       "          [0.6078, 0.5922, 0.6196,  ..., 0.3176, 0.2863, 0.3529]]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'boxes': tensor([[565., 212., 599., 248.]]),\n",
       "  'labels': tensor([1]),\n",
       "  'image_id': tensor([141]),\n",
       "  'area': tensor([1224.]),\n",
       "  'iscrowd': tensor([0])},)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-4207c5ca94a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mimage_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model(imgs, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[1.6724e+02, 1.0477e-01, 1.8248e+02, 1.3433e+01],\n",
       "          [1.6714e+02, 6.1300e+01, 1.7843e+02, 8.2150e+01],\n",
       "          [1.7090e+02, 6.1205e+01, 1.8216e+02, 8.2248e+01],\n",
       "          [5.9814e+01, 1.0219e+02, 8.2420e+01, 1.1313e+02],\n",
       "          [1.2127e+02, 9.0735e+01, 1.4362e+02, 1.0161e+02],\n",
       "          [1.0206e+02, 5.6015e+01, 1.1328e+02, 6.1356e+01],\n",
       "          [1.0205e+02, 5.7960e+01, 1.1325e+02, 6.3280e+01],\n",
       "          [5.9640e+01, 1.0611e+02, 8.2102e+01, 1.1698e+02],\n",
       "          [1.0202e+02, 5.3115e+01, 1.1326e+02, 5.8458e+01],\n",
       "          [1.0693e+02, 5.7974e+01, 1.1806e+02, 6.3287e+01],\n",
       "          [1.2927e+02, 8.3020e+01, 1.5118e+02, 9.3930e+01],\n",
       "          [1.0046e+02, 1.1409e-01, 1.4488e+02, 1.4135e+01],\n",
       "          [1.7934e+02, 9.5599e-02, 1.9200e+02, 1.5452e+01],\n",
       "          [1.6144e+02, 6.1394e+01, 1.7249e+02, 8.2472e+01],\n",
       "          [1.6520e+02, 9.1929e+00, 1.8068e+02, 2.4634e+01],\n",
       "          [5.9638e+01, 8.4594e+01, 8.2234e+01, 9.5837e+01],\n",
       "          [1.5399e+02, 1.4245e+02, 1.6515e+02, 1.4771e+02],\n",
       "          [1.6901e+02, 5.5337e+01, 1.8032e+02, 7.6457e+01],\n",
       "          [1.0948e+02, 1.1959e+02, 1.3177e+02, 1.3039e+02],\n",
       "          [1.0973e+02, 6.3686e+01, 1.2088e+02, 6.8992e+01],\n",
       "          [9.9208e+01, 5.5079e+01, 1.1038e+02, 6.0422e+01],\n",
       "          [1.1730e+02, 1.1377e+02, 1.3965e+02, 1.2460e+02],\n",
       "          [5.1947e+01, 7.9001e+01, 6.3253e+01, 8.4290e+01],\n",
       "          [1.0202e+02, 5.9326e-02, 1.1324e+02, 1.0835e+01],\n",
       "          [1.4596e+02, 2.0551e-01, 1.7715e+02, 2.2660e+01],\n",
       "          [7.7908e+01, 1.0491e+02, 8.9134e+01, 1.1023e+02],\n",
       "          [1.0690e+02, 5.9898e+01, 1.1804e+02, 6.5204e+01],\n",
       "          [1.0302e+02, 5.9872e+01, 1.1421e+02, 6.5197e+01],\n",
       "          [4.4160e+01, 7.6268e+01, 5.5482e+01, 9.7377e+01],\n",
       "          [1.6999e+02, 1.8083e+02, 1.8125e+02, 1.8610e+02],\n",
       "          [5.9784e+01, 1.2516e+02, 7.1009e+01, 1.3056e+02],\n",
       "          [5.9365e+01, 8.8417e+01, 8.2372e+01, 9.9598e+01],\n",
       "          [1.7015e+02, 1.8661e+02, 1.8146e+02, 1.9189e+02],\n",
       "          [1.3439e+02, 1.7687e-01, 1.6529e+02, 1.9019e+01],\n",
       "          [1.0977e+02, 6.0816e+01, 1.2091e+02, 6.6119e+01],\n",
       "          [9.7294e+01, 5.7002e+01, 1.0846e+02, 6.2321e+01],\n",
       "          [1.0410e+02, 1.1550e+02, 1.2590e+02, 1.2640e+02],\n",
       "          [1.5634e+01, 1.4210e+02, 3.0962e+01, 1.5761e+02],\n",
       "          [1.0850e+02, 1.1938e+02, 1.1981e+02, 1.2469e+02],\n",
       "          [1.3835e+02, 1.5389e+01, 1.5366e+02, 3.1031e+01],\n",
       "          [3.2795e+01, 3.6356e+01, 4.3973e+01, 5.7096e+01],\n",
       "          [9.0261e+01, 3.2221e-01, 1.1232e+02, 1.1378e+01],\n",
       "          [1.0686e+02, 5.6062e+01, 1.1806e+02, 6.1412e+01],\n",
       "          [1.7791e+02, 2.0007e+02, 1.8910e+02, 2.0538e+02],\n",
       "          [1.0983e+02, 5.8902e+01, 1.2097e+02, 6.4216e+01],\n",
       "          [0.0000e+00, 1.7510e+02, 1.8555e+01, 1.8616e+02],\n",
       "          [1.1929e+02, 7.1453e+01, 1.4127e+02, 1.1351e+02],\n",
       "          [1.1914e+02, 4.6315e+01, 1.3455e+02, 6.1721e+01],\n",
       "          [4.9081e+01, 7.7103e+01, 6.0389e+01, 8.2364e+01],\n",
       "          [1.3654e+01, 2.4808e+02, 2.5037e+01, 2.5340e+02],\n",
       "          [1.0761e+02, 4.6293e+01, 1.2305e+02, 6.1698e+01],\n",
       "          [4.2251e+01, 7.9862e+01, 5.3499e+01, 8.5199e+01],\n",
       "          [4.5313e+01, 7.8947e+01, 5.6501e+01, 8.4265e+01],\n",
       "          [1.2344e+02, 1.1749e+02, 1.4564e+02, 1.2821e+02],\n",
       "          [1.7015e+02, 1.8374e+02, 1.8141e+02, 1.8898e+02],\n",
       "          [1.6162e+02, 1.4335e+02, 1.7283e+02, 1.4864e+02],\n",
       "          [1.7687e+02, 1.9716e+02, 1.8806e+02, 2.0244e+02],\n",
       "          [1.6326e+02, 5.5432e+01, 1.7452e+02, 7.6593e+01],\n",
       "          [2.5060e+01, 1.6150e+02, 4.0440e+01, 1.7690e+02],\n",
       "          [9.6366e+01, 1.2314e+02, 1.1837e+02, 1.3402e+02],\n",
       "          [1.2532e+02, 9.4712e+01, 1.4756e+02, 1.0558e+02],\n",
       "          [7.8804e+01, 6.2044e-02, 8.9929e+01, 1.0923e+01],\n",
       "          [1.7089e+02, 1.7510e+02, 1.8223e+02, 1.8037e+02],\n",
       "          [1.3124e+02, 1.7782e-01, 1.5341e+02, 2.5233e+01],\n",
       "          [3.3875e+01, 1.6835e+02, 4.5089e+01, 1.7372e+02],\n",
       "          [9.4206e+01, 1.2297e+02, 1.0972e+02, 1.3843e+02],\n",
       "          [1.0754e+02, 1.2336e+02, 1.2993e+02, 1.3413e+02],\n",
       "          [1.0419e+02, 8.3224e+01, 1.2627e+02, 1.2572e+02],\n",
       "          [4.2256e+01, 7.6068e+01, 5.3550e+01, 8.1396e+01],\n",
       "          [0.0000e+00, 1.4948e+02, 2.3536e+01, 1.8018e+02],\n",
       "          [6.7673e+01, 1.0789e+02, 8.9960e+01, 1.1903e+02],\n",
       "          [2.1315e+01, 2.2313e+01, 3.2474e+01, 4.3757e+01],\n",
       "          [1.4813e+02, 1.3296e+02, 1.7021e+02, 1.4382e+02],\n",
       "          [5.9862e+01, 8.0662e+01, 8.2416e+01, 9.1905e+01],\n",
       "          [1.4264e+02, 1.3495e+02, 1.6469e+02, 1.4574e+02],\n",
       "          [4.6255e+01, 1.3283e+02, 6.1774e+01, 1.4817e+02],\n",
       "          [1.6335e+02, 4.3460e+01, 1.7443e+02, 6.4834e+01],\n",
       "          [5.3837e+01, 7.6170e+01, 6.5229e+01, 8.1438e+01],\n",
       "          [1.2118e+01, 2.5009e+02, 3.4048e+01, 2.5600e+02],\n",
       "          [1.7574e+02, 1.8273e+02, 1.8704e+02, 1.8808e+02],\n",
       "          [1.2354e+02, 7.3258e+01, 1.4536e+02, 8.4285e+01],\n",
       "          [1.7521e+02, 6.0107e-02, 1.9050e+02, 7.6689e+00],\n",
       "          [1.1224e+02, 8.3141e+01, 1.3414e+02, 1.2588e+02],\n",
       "          [5.5935e+01, 5.1267e+00, 6.7130e+01, 2.6443e+01],\n",
       "          [2.5255e+01, 3.0147e+01, 3.6499e+01, 5.1193e+01],\n",
       "          [9.6561e+01, 1.5217e+02, 1.1847e+02, 1.6301e+02],\n",
       "          [9.2046e+01, 8.2159e+01, 1.3666e+02, 1.0304e+02],\n",
       "          [5.9872e+01, 7.6136e+01, 7.1006e+01, 9.7283e+01],\n",
       "          [1.0394e+02, 6.6559e+01, 1.1507e+02, 7.1876e+01],\n",
       "          [4.4370e+01, 8.7607e+01, 5.5473e+01, 9.2892e+01],\n",
       "          [9.5926e+01, 3.8684e+01, 1.1149e+02, 5.4109e+01],\n",
       "          [0.0000e+00, 1.7128e+02, 1.8572e+01, 1.8237e+02],\n",
       "          [4.5170e+01, 8.0774e+01, 5.6468e+01, 8.6115e+01],\n",
       "          [5.9557e+01, 1.2294e+02, 7.5061e+01, 1.3838e+02],\n",
       "          [7.7151e+01, 1.2475e+02, 8.2511e+01, 1.3558e+02],\n",
       "          [1.5940e+02, 5.5445e+01, 1.7060e+02, 7.6631e+01],\n",
       "          [6.6649e+01, 8.8078e+01, 7.1896e+01, 9.8700e+01],\n",
       "          [1.6519e+02, 9.6110e+01, 1.8053e+02, 1.1142e+02],\n",
       "          [3.0028e+01, 1.5007e+02, 4.1355e+01, 1.5546e+02],\n",
       "          [1.1553e+02, 6.2765e+01, 1.2669e+02, 6.8058e+01]], device='cuda:0'),\n",
       "  'labels': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2], device='cuda:0'),\n",
       "  'scores': tensor([0.3508, 0.3483, 0.3482, 0.3461, 0.3461, 0.3433, 0.3427, 0.3423, 0.3419,\n",
       "          0.3419, 0.3416, 0.3415, 0.3415, 0.3413, 0.3413, 0.3413, 0.3412, 0.3411,\n",
       "          0.3411, 0.3410, 0.3410, 0.3409, 0.3409, 0.3408, 0.3408, 0.3405, 0.3405,\n",
       "          0.3404, 0.3404, 0.3403, 0.3403, 0.3402, 0.3402, 0.3402, 0.3401, 0.3401,\n",
       "          0.3400, 0.3400, 0.3399, 0.3399, 0.3398, 0.3398, 0.3397, 0.3397, 0.3396,\n",
       "          0.3396, 0.3395, 0.3394, 0.3394, 0.3394, 0.3394, 0.3393, 0.3393, 0.3393,\n",
       "          0.3392, 0.3392, 0.3392, 0.3392, 0.3392, 0.3391, 0.3391, 0.3391, 0.3391,\n",
       "          0.3391, 0.3390, 0.3390, 0.3389, 0.3389, 0.3389, 0.3389, 0.3389, 0.3388,\n",
       "          0.3388, 0.3388, 0.3388, 0.3388, 0.3387, 0.3386, 0.3386, 0.3385, 0.3384,\n",
       "          0.3383, 0.3383, 0.3383, 0.3383, 0.3382, 0.3382, 0.3382, 0.3381, 0.3381,\n",
       "          0.3381, 0.3381, 0.3381, 0.3380, 0.3380, 0.3380, 0.3379, 0.3379, 0.3379,\n",
       "          0.3379], device='cuda:0')}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[114., 127., 232., 287.],\n",
      "        [306., 123., 419., 255.],\n",
      "        [751.,  80., 856., 187.]], device='cuda:0'), 'labels': tensor([1, 1, 1], device='cuda:0'), 'image_id': tensor([89], device='cuda:0'), 'area': tensor([18880., 14916., 11235.], device='cuda:0'), 'iscrowd': tensor([0, 0, 0], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for imgs, annotations in data_loader:\n",
    "    i += 1\n",
    "    if i % 2 == 0:\n",
    "        break\n",
    "    imgs = [img.to(device) for img in imgs]\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2314, 0.2431, 0.2510,  ..., 0.4431, 0.4392, 0.4353],\n",
       "          [0.2235, 0.2275, 0.2235,  ..., 0.4471, 0.4431, 0.4392],\n",
       "          [0.2275, 0.2275, 0.2353,  ..., 0.4471, 0.4431, 0.4392],\n",
       "          ...,\n",
       "          [0.1608, 0.1725, 0.1569,  ..., 0.6980, 0.7333, 0.7059],\n",
       "          [0.1490, 0.1608, 0.1647,  ..., 0.7098, 0.7882, 0.6078],\n",
       "          [0.1608, 0.1333, 0.1843,  ..., 0.6275, 0.7373, 0.5882]],\n",
       " \n",
       "         [[0.2235, 0.2235, 0.2157,  ..., 0.2549, 0.2510, 0.2471],\n",
       "          [0.1961, 0.1922, 0.1765,  ..., 0.2588, 0.2549, 0.2510],\n",
       "          [0.1843, 0.1843, 0.1882,  ..., 0.2588, 0.2549, 0.2510],\n",
       "          ...,\n",
       "          [0.1294, 0.1412, 0.1176,  ..., 0.7608, 0.7804, 0.7451],\n",
       "          [0.1176, 0.1294, 0.1255,  ..., 0.7804, 0.8431, 0.6471],\n",
       "          [0.1294, 0.1020, 0.1451,  ..., 0.6980, 0.7922, 0.6275]],\n",
       " \n",
       "         [[0.2353, 0.2118, 0.1804,  ..., 0.1686, 0.1647, 0.1608],\n",
       "          [0.2275, 0.2118, 0.1765,  ..., 0.1725, 0.1686, 0.1647],\n",
       "          [0.2157, 0.2078, 0.2039,  ..., 0.1725, 0.1686, 0.1647],\n",
       "          ...,\n",
       "          [0.1176, 0.1294, 0.1098,  ..., 0.8118, 0.8275, 0.7843],\n",
       "          [0.1059, 0.1176, 0.1176,  ..., 0.8275, 0.8863, 0.6941],\n",
       "          [0.1176, 0.0902, 0.1373,  ..., 0.7529, 0.8353, 0.6745]]]),)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'boxes': tensor([[ 37.,  39.,  55.,  60.],\n",
       "          [ 92.,  28., 115.,  49.],\n",
       "          [143.,  17., 180.,  57.],\n",
       "          [192.,   6., 209.,  25.],\n",
       "          [169.,  90., 255., 192.]]),\n",
       "  'labels': tensor([1, 1, 1, 1, 1]),\n",
       "  'image_id': tensor([543]),\n",
       "  'area': tensor([ 378.,  483., 1480.,  323., 8772.]),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0])},)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_test,annotation_test = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_test = list(img.type(torch.FloatTensor).to(device) for img in imgs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devesh/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model(imgs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [{k: v.to('cpu') for k, v in t.items()} for t in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([], device='cuda:0', size=(0, 4)),\n",
       "  'labels': tensor([], device='cuda:0', dtype=torch.int64),\n",
       "  'scores': tensor([], device='cuda:0')}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'boxes': tensor([[190., 190., 421., 483.]]),\n",
       "  'labels': tensor([1]),\n",
       "  'image_id': tensor([1]),\n",
       "  'area': tensor([67683.]),\n",
       "  'iscrowd': tensor([0])},)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.ops.nms(prediction[0]['boxes'], prediction[0]['scores'], 0.3).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([164.70686, 165.7026 , 436.88776, 503.93878], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(prediction[0]['boxes'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([190., 190., 421., 483.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_test[0]['boxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
    "\txA = max(boxA[0], boxB[0])\n",
    "\tyA = max(boxA[1], boxB[1])\n",
    "\txB = min(boxA[2], boxB[2])\n",
    "\tyB = min(boxA[3], boxB[3])\n",
    "\t# compute the area of intersection rectangle\n",
    "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\t# compute the area of both the prediction and ground-truth\n",
    "\t# rectangles\n",
    "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\t# compute the intersection over union by taking the intersection\n",
    "\t# area and dividing it by the sum of prediction + ground-truth\n",
    "\t# areas - the interesection area\n",
    "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\t# return the intersection over union value\n",
    "\treturn iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.736008470172147"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_intersection_over_union(np.array(annotation_test[0]['boxes'][0]), np.array(prediction[0]['boxes'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(val_coco) as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_map_id={}\n",
    "for x in data[\"images\"]:\n",
    "  test_images_map_id[x[\"file_name\"]]=x[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for img, img_ids in test_loader:\n",
    "    i += 1\n",
    "    if i % 5 == 0:\n",
    "        break\n",
    "    print(img_ids[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[0.1843, 0.1843, 0.1843,  ..., 0.1843, 0.1843, 0.1843],\n",
       "           [0.1843, 0.1843, 0.1843,  ..., 0.1843, 0.1843, 0.1843],\n",
       "           [0.1843, 0.1843, 0.1843,  ..., 0.1843, 0.1843, 0.1843],\n",
       "           ...,\n",
       "           [0.1922, 0.1922, 0.1922,  ..., 0.1804, 0.1804, 0.1804],\n",
       "           [0.1843, 0.1843, 0.1843,  ..., 0.1843, 0.1843, 0.1843],\n",
       "           [0.1843, 0.1843, 0.1843,  ..., 0.1922, 0.1922, 0.1922]],\n",
       "  \n",
       "          [[0.6902, 0.6902, 0.6902,  ..., 0.6902, 0.6902, 0.6902],\n",
       "           [0.6902, 0.6902, 0.6902,  ..., 0.6902, 0.6902, 0.6902],\n",
       "           [0.6902, 0.6902, 0.6902,  ..., 0.6902, 0.6902, 0.6902],\n",
       "           ...,\n",
       "           [0.6863, 0.6863, 0.6863,  ..., 0.6902, 0.6902, 0.6902],\n",
       "           [0.6941, 0.6941, 0.6941,  ..., 0.6902, 0.6902, 0.6902],\n",
       "           [0.6902, 0.6902, 0.6902,  ..., 0.6863, 0.6863, 0.6863]],\n",
       "  \n",
       "          [[0.9137, 0.9137, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n",
       "           [0.9137, 0.9137, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n",
       "           [0.9137, 0.9137, 0.9137,  ..., 0.9137, 0.9137, 0.9137],\n",
       "           ...,\n",
       "           [0.9137, 0.9137, 0.9137,  ..., 0.9176, 0.9176, 0.9176],\n",
       "           [0.8980, 0.8980, 0.8980,  ..., 0.9137, 0.9137, 0.9137],\n",
       "           [0.9059, 0.9059, 0.9059,  ..., 0.9176, 0.9176, 0.9176]]],\n",
       "         dtype=torch.float64),),\n",
       " (tensor([0]),))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "detection_threshold = 0.9\n",
    "results = []\n",
    "\n",
    "for images, image_ids in test_loader:\n",
    "\n",
    "    images = list(image.type(torch.FloatTensor).to(device) for image in images)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        category = outputs[i]['labels'].data.cpu().numpy()\n",
    "        \n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.float32)\n",
    "        scores = scores[scores >= detection_threshold]\n",
    "        image_id = image_ids[0].item()\n",
    "        category = category[:len(boxes)].astype(np.int32)\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        \n",
    "        result = {\n",
    "            'image_id': int(image_id),\n",
    "            'category_id': category.tolist(),\n",
    "            'bbox': boxes.tolist(),\n",
    "            'score': scores.tolist()\n",
    "        }\n",
    "\n",
    "        del(images)\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 0, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 1, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 2, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 3, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 4, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 5, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 6, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 7, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 8, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 9, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 10, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 11, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 12, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 13, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 14, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 15, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 16, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 17, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 18, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 19, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 20, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 21, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 22, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 23, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 24, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 25, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 26, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 27, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 28, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 29, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 30, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 31, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 32, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 33, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 34, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 35, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 36, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 37, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 38, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 39, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 40, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 41, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 42, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 43, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 44, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 45, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 46, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 47, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 48, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 49, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 50, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 51, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 52, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 53, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 54, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 55, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 56, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 57, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 58, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 59, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 60, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 61, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 62, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 63, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 64, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 65, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 66, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 67, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 68, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 69, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 70, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 71, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 72, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 73, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 74, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 75, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 76, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 77, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 78, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 79, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 80, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 81, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 82, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 83, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 84, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 85, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 86, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 87, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 88, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 89, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 90, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 91, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 92, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 93, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 94, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 95, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 96, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 97, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 98, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 99, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 100, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 101, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 102, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 103, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 104, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 105, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 106, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 107, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 108, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 109, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 110, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 111, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 112, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 113, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 114, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 115, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 116, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 117, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 118, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 119, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 120, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 121, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 122, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 123, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 124, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 125, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 126, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 127, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 128, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 129, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 130, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 131, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 132, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 133, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 134, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 135, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 136, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 137, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 138, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 139, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 140, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 141, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 142, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 143, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 144, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 145, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 146, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 147, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 148, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 149, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 150, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 151, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 152, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 153, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 154, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 155, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 156, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 157, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 158, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 159, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 160, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 161, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 162, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 163, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 164, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 165, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 166, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 167, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 168, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 169, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 170, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 171, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 172, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 173, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 174, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 175, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 176, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 177, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 178, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 179, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 180, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 181, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 182, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 183, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 184, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 185, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 186, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 187, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 188, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 189, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 190, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 191, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 192, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 193, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 194, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 195, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 196, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 197, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 198, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 199, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 200, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 201, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 202, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 203, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 204, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 205, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 206, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 207, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 208, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 209, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 210, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 211, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 212, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 213, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 214, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 215, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 216, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 217, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 218, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 219, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 220, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 221, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 222, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 223, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 224, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 225, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 226, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 227, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 228, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 229, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 230, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 231, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 232, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 233, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 234, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 235, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 236, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 237, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 238, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 239, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 240, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 241, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 242, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 243, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 244, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 245, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 246, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 247, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 248, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 249, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 250, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 251, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 252, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 253, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 254, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 255, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 256, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 257, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 258, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 259, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 260, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 261, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 262, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 263, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 264, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 265, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 266, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 267, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 268, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 269, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 270, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 271, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 272, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 273, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 274, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 275, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 276, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 277, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 278, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 279, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 280, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 281, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 282, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 283, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 284, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 285, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 286, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 287, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 288, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 289, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 290, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 291, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 292, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 293, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 294, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 295, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 296, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 297, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 298, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 299, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 300, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 301, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 302, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 303, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 304, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 305, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 306, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 307, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 308, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 309, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 310, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 311, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 312, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 313, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 314, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 315, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 316, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 317, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 318, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 319, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 320, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 321, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 322, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 323, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 324, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 325, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 326, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 327, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 328, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 329, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 330, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 331, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 332, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 333, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 334, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 335, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 336, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 337, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 338, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 339, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 340, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 341, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 342, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 343, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 344, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 345, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 346, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 347, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 348, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 349, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 350, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 351, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 352, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 353, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 354, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 355, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 356, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 357, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 358, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 359, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 360, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 361, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 362, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 363, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 364, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 365, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 366, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 367, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 368, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 369, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 370, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 371, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 372, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 373, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 374, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 375, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 376, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 377, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 378, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 379, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 380, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 381, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 382, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 383, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 384, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 385, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 386, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 387, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 388, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 389, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 390, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 391, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 392, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 393, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 394, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 395, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 396, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 397, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 398, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 399, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 400, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 401, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 402, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 403, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 404, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 405, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 406, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 407, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 408, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 409, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 410, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 411, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 412, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 413, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 414, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 415, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 416, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 417, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 418, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 419, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 420, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 421, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 422, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 423, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 424, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 425, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 426, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 427, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 428, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 429, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 430, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 431, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 432, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 433, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 434, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 435, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 436, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 437, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 438, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 439, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 440, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 441, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 442, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 443, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 444, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 445, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 446, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 447, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 448, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 449, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 450, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 451, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 452, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 453, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 454, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 455, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 456, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 457, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 458, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 459, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 460, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 461, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 462, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 463, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 464, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 465, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 466, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 467, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 468, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 469, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 470, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 471, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 472, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 473, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 474, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 475, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 476, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 477, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 478, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 479, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 480, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 481, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 482, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 483, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 484, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 485, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 486, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 487, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 488, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 489, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 490, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 491, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 492, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 493, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 494, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 495, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 496, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 497, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 498, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 499, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 500, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 501, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 502, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 503, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 504, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 505, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 506, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 507, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 508, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 509, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 510, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 511, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 512, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 513, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 514, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 515, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 516, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 517, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 518, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 519, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 520, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 521, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 522, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 523, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 524, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 525, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 526, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 527, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 528, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 529, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 530, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 531, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 532, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 533, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 534, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 535, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 536, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 537, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 538, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 539, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 540, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 541, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 542, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 543, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 544, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 545, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 546, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 547, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 548, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 549, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 550, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 551, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 552, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 553, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 554, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 555, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 556, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 557, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 558, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 559, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 560, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 561, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 562, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 563, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 564, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 565, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 566, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 567, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 568, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 569, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 570, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 571, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 572, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 573, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 574, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 575, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 576, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 577, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 578, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 579, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 580, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 581, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 582, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 583, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 584, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 585, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 586, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 587, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 588, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 589, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 590, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 591, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 592, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 593, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 594, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 595, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 596, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 597, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 598, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 599, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 600, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 601, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 602, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 603, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 604, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 605, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 606, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 607, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 608, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 609, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 610, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 611, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 612, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 613, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 614, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 615, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 616, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 617, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 618, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 619, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 620, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 621, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 622, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 623, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 624, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 625, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 626, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 627, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 628, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 629, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 630, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 631, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 632, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 633, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 634, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 635, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 636, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 637, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 638, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 639, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 640, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 641, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 642, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 643, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 644, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 645, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 646, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 647, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 648, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 649, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 650, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 651, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 652, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 653, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 654, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 655, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 656, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 657, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 658, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 659, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 660, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 661, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 662, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 663, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 664, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 665, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 666, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 667, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 668, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 669, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 670, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 671, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 672, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 673, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 674, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 675, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 676, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 677, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 678, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 679, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 680, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 681, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 682, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 683, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 684, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 685, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 686, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 687, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 688, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 689, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 690, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 691, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 692, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 693, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 694, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 695, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 696, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 697, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 698, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 699, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 700, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 701, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 702, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 703, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 704, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 705, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 706, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 707, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 708, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 709, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 710, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 711, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 712, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 713, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 714, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 715, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 716, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 717, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 718, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 719, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 720, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 721, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 722, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 723, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 724, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 725, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 726, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 727, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 728, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 729, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 730, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 731, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 732, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 733, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 734, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 735, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 736, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 737, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 738, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 739, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 740, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 741, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 742, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 743, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 744, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 745, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 746, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 747, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 748, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 749, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 750, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 751, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 752, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 753, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 754, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 755, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 756, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 757, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 758, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 759, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 760, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 761, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 762, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 763, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 764, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 765, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 766, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 767, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 768, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 769, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 770, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 771, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 772, 'category_id': [], 'bbox': [], 'score': []},\n",
       " {'image_id': 773, 'category_id': [], 'bbox': [], 'score': []}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.6431, 0.6549, 0.6549],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6471, 0.6706, 0.6314],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.6549, 0.6627, 0.6314],\n",
       "           ...,\n",
       "           [0.5294, 0.5137, 0.5255,  ..., 0.8118, 0.8078, 0.8039],\n",
       "           [0.5451, 0.5255, 0.5216,  ..., 0.8157, 0.8157, 0.8118],\n",
       "           [0.5569, 0.5216, 0.5490,  ..., 0.8275, 0.8353, 0.8314]],\n",
       "  \n",
       "          [[1.0000, 1.0000, 1.0000,  ..., 0.3608, 0.3725, 0.3765],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.3647, 0.3882, 0.3529],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.3725, 0.3843, 0.3529],\n",
       "           ...,\n",
       "           [0.4314, 0.4157, 0.4275,  ..., 0.6392, 0.6353, 0.6314],\n",
       "           [0.4471, 0.4275, 0.4235,  ..., 0.6431, 0.6431, 0.6392],\n",
       "           [0.4588, 0.4235, 0.4510,  ..., 0.6549, 0.6627, 0.6588]],\n",
       "  \n",
       "          [[1.0000, 1.0000, 1.0000,  ..., 0.3647, 0.3765, 0.3686],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.3686, 0.3922, 0.3451],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.3765, 0.3765, 0.3451],\n",
       "           ...,\n",
       "           [0.4431, 0.4275, 0.4392,  ..., 0.5333, 0.5294, 0.5255],\n",
       "           [0.4588, 0.4392, 0.4353,  ..., 0.5373, 0.5373, 0.5333],\n",
       "           [0.4745, 0.4353, 0.4627,  ..., 0.5490, 0.5569, 0.5529]]]),),\n",
       " ({'boxes': tensor([[190., 190., 421., 483.]]),\n",
       "   'labels': tensor([1]),\n",
       "   'image_id': tensor([1]),\n",
       "   'area': tensor([67683.]),\n",
       "   'iscrowd': tensor([0])},))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON...\n"
     ]
    }
   ],
   "source": [
    "fp = open('/run/media/devesh/Hard Disk/Projects/AI Crowd Blitz 2/MASKD/sub/try.json', \"w\")\n",
    "import json\n",
    "print(\"Writing JSON...\")\n",
    "fp.write(json.dumps(results))\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = 0\n",
    "new_result = []\n",
    "for result in results:\n",
    "#     j += 1\n",
    "#     if j % 5 == 0:\n",
    "#         break\n",
    "    for i in range(len(result['bbox'])):\n",
    "        x = {\n",
    "            'image_id': int(result['image_id']),\n",
    "            'category_id': result['category_id'][i],\n",
    "            'bbox': result['bbox'][i],\n",
    "            'score': result['score'][i]\n",
    "        }\n",
    "        new_result.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON...\n"
     ]
    }
   ],
   "source": [
    "fp = open('/run/media/devesh/Hard Disk/Projects/AI Crowd Blitz 2/MASKD/sub/sub_2.json', \"w\")\n",
    "import json\n",
    "print(\"Writing JSON...\")\n",
    "fp.write(json.dumps(new_result))\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
